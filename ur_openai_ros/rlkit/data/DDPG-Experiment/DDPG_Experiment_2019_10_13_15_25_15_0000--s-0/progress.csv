evaluation/num steps total,exploration/Actions Min,exploration/Rewards Std,trainer/Q Predictions Std,time/evaluation sampling (s),trainer/QF Loss,evaluation/env_infos/final/total_distance Std,exploration/env_infos/total_distance Mean,evaluation/Rewards Max,evaluation/Returns Max,exploration/path length Min,exploration/Num Paths,trainer/Bellman Errors Mean,evaluation/Returns Min,exploration/Actions Mean,exploration/env_infos/final/total_distance Min,exploration/env_infos/total_distance Std,trainer/Bellman Errors Std,exploration/Returns Max,evaluation/Rewards Std,exploration/Returns Min,evaluation/num paths total,trainer/Bellman Errors Max,trainer/Raw Policy Loss,evaluation/path length Min,evaluation/Returns Std,time/exploration sampling (s),trainer/Bellman Errors Min,time/saving (s),exploration/env_infos/total_distance Max,exploration/num paths total,trainer/Policy Action Std,trainer/Q Targets Max,Epoch,evaluation/env_infos/initial/total_distance Max,trainer/Q Targets Mean,trainer/Q Targets Std,evaluation/env_infos/final/total_distance Max,evaluation/Actions Mean,exploration/env_infos/initial/total_distance Std,evaluation/path length Mean,exploration/Rewards Max,time/epoch (s),exploration/path length Mean,exploration/Rewards Min,evaluation/Rewards Min,time/training (s),evaluation/Actions Std,trainer/Policy Loss,evaluation/path length Max,time/data storing (s),evaluation/env_infos/final/total_distance Mean,evaluation/Num Paths,exploration/env_infos/final/total_distance Std,evaluation/env_infos/total_distance Max,trainer/Q Predictions Min,evaluation/Returns Mean,exploration/Returns Mean,exploration/env_infos/initial/total_distance Min,trainer/Q Predictions Mean,exploration/path length Max,exploration/Returns Std,evaluation/env_infos/total_distance Std,exploration/num steps total,trainer/Q Targets Min,trainer/Policy Action Max,evaluation/env_infos/initial/total_distance Mean,evaluation/env_infos/total_distance Min,trainer/Q Predictions Max,exploration/Average Returns,trainer/Policy Action Min,exploration/Rewards Mean,exploration/Actions Std,exploration/env_infos/final/total_distance Max,evaluation/env_infos/final/total_distance Min,exploration/env_infos/initial/total_distance Mean,exploration/path length Std,evaluation/Actions Max,time/total (s),evaluation/path length Std,exploration/env_infos/total_distance Min,evaluation/env_infos/total_distance Mean,evaluation/env_infos/initial/total_distance Std,time/logging (s),trainer/Preactivation Policy Loss,replay_buffer/size,evaluation/Average Returns,exploration/env_infos/initial/total_distance Max,trainer/Policy Action Mean,exploration/env_infos/final/total_distance Mean,exploration/Actions Max,evaluation/Actions Min,evaluation/env_infos/initial/total_distance Min,evaluation/Rewards Mean
1,-0.349885788983,0.0,0.000155518,1.2695639799931087,0.7626,0.0,0.935913052694,-1.17776041536,-1.17776041536,1,1,0.7626,-1.17776041536,-0.0901817282614,0.935913052694,0.0,0.00369631,-0.875933242204,0.0,-0.875933242204,1,0.766474,0.00455997,1,0.0,1.2610998669988476,0.759073,0.17594720900524408,0.935913052694,2,0.00382119,-0.875933,0,1.08524670714,-0.878101,0.00227166,1.08524670714,0.00074511,0.0,1.0,-0.875933242204,2.7202997840067837,1.0,-0.875933242204,-1.17776041536,0.011297837016172707,0.00382119,0.00455997,1,4.603198613040149e-05,1.08524670714,1,0.0,1.08524670714,-0.00499647,-1.17776041536,-0.875933242204,0.935913052694,-0.00483348,1,0.0,0.0,2,-0.880482,0.00512939,1.08524670714,1.08524670714,-0.00468509,-0.875933242204,-0.00600678,-0.875933242204,0.205096435197,0.935913052694,1.08524670714,0.935913052694,0.0,0.00512939,7.320817406027345,0.0,0.935913052694,1.08524670714,0.0,0.00234485900728032,0.0,2,-1.17776041536,0.935913052694,0.00074511,0.935913052694,0.209588018404,-0.00600678,1.08524670714,-1.17776041536
