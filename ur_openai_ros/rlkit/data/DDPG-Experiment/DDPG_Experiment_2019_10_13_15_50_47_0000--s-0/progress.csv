replay_buffer/size,exploration/Actions Std,evaluation/Actions Mean,trainer/Q Targets Min,trainer/QF Loss,exploration/env_infos/final/total_distance Mean,exploration/env_infos/initial/total_distance Min,exploration/Actions Min,exploration/env_infos/initial/total_distance Mean,evaluation/Rewards Min,exploration/Rewards Std,evaluation/Actions Min,trainer/Raw Policy Loss,evaluation/num steps total,evaluation/path length Std,evaluation/Rewards Max,time/exploration sampling (s),trainer/Q Predictions Min,evaluation/env_infos/total_distance Min,evaluation/env_infos/final/total_distance Max,trainer/Preactivation Policy Loss,evaluation/path length Max,evaluation/Num Paths,exploration/env_infos/final/total_distance Max,trainer/Q Predictions Std,trainer/Policy Action Mean,evaluation/Returns Std,exploration/Num Paths,exploration/env_infos/initial/total_distance Std,evaluation/env_infos/total_distance Max,trainer/Policy Action Max,exploration/Actions Max,time/logging (s),trainer/Q Predictions Mean,trainer/Bellman Errors Max,exploration/env_infos/final/total_distance Min,exploration/Returns Mean,evaluation/env_infos/total_distance Mean,trainer/Bellman Errors Std,exploration/env_infos/total_distance Std,exploration/env_infos/total_distance Max,exploration/env_infos/total_distance Min,exploration/path length Std,exploration/path length Min,time/epoch (s),exploration/num steps total,exploration/Rewards Min,exploration/Returns Min,evaluation/env_infos/final/total_distance Mean,evaluation/Rewards Std,time/training (s),exploration/Rewards Mean,exploration/num paths total,trainer/Bellman Errors Mean,evaluation/env_infos/final/total_distance Min,evaluation/path length Min,trainer/Bellman Errors Min,evaluation/env_infos/total_distance Std,evaluation/path length Mean,exploration/env_infos/initial/total_distance Max,trainer/Policy Action Std,evaluation/env_infos/initial/total_distance Min,evaluation/env_infos/initial/total_distance Mean,time/saving (s),exploration/path length Mean,exploration/env_infos/total_distance Mean,exploration/Rewards Max,evaluation/Returns Max,trainer/Q Predictions Max,evaluation/Actions Std,trainer/Policy Loss,Epoch,evaluation/Average Returns,trainer/Policy Action Min,exploration/Actions Mean,evaluation/env_infos/initial/total_distance Max,evaluation/env_infos/initial/total_distance Std,time/total (s),exploration/env_infos/final/total_distance Std,exploration/Returns Std,exploration/path length Max,trainer/Q Targets Max,exploration/Average Returns,time/evaluation sampling (s),time/data storing (s),trainer/Q Targets Std,trainer/Q Targets Mean,evaluation/num paths total,evaluation/Returns Mean,exploration/Returns Max,evaluation/Rewards Mean,evaluation/Returns Min,evaluation/env_infos/final/total_distance Std,evaluation/Actions Max
2000,0.300721893225,0.000756562,-0.666702,0.446406,0.815911920816,0.810270435455,-1.0,0.815911920816,-0.905947358502,0.000563300150516,-0.00221293,-0.0036709,1000,0.0,-0.904929545276,1317.0207112350035,0.00240287,0.951277848621,0.951812669858,0.0,1,1000,0.816699901682,0.00039724,0.000757162,0.000424268909462,1000,0.0003455787722,0.951812669858,0.00604941,0.990762209708,0.029894636012613773,0.00367189,0.450077,0.810270435455,-0.665712381955,0.951462270088,0.00247507,0.0003455787722,0.816699901682,0.810270435455,0.0,1,2606.6670703729906,2000,-0.666998729408,-0.666998729408,0.951462270088,0.000424268909462,5.111996688996442,-0.665712381955,2000,0.446406,0.951277848621,1,0.442194,0.000222935769229,1.0,0.816699901682,0.00277494,0.951277848621,0.951462270088,0.1177250600012485,1.0,0.815911920816,-0.656538178572,-0.904929545276,0.00448276,0.00277493,-0.0036709,0,-0.905280501102,-0.00220784,0.00215928277755,0.951812669858,0.000222935769229,3886.627837857988,0.0003455787722,0.000563300150516,1,-0.661459,-0.665712381955,1284.377696095995,0.009046656981809065,0.00184741,-0.664461,1000,-0.905280501102,-0.656538178572,-0.905280501102,-0.905947358502,0.000222935769229,0.00604939
