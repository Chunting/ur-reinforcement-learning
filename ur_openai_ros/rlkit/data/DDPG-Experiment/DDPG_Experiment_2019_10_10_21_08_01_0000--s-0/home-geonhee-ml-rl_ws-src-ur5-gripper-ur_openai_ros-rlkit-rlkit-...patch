diff --git a/rlkit/samplers/rollout_functions.py b/rlkit/samplers/rollout_functions.py
index 0733983..c136fb5 100644
--- a/rlkit/samplers/rollout_functions.py
+++ b/rlkit/samplers/rollout_functions.py
@@ -103,12 +103,14 @@ def rollout(
     agent_infos = []
     env_infos = []
     o = env.reset()
+    print ("sampler/rollout_functions.py, rollout, o = env.reset(): ", type(o))
     agent.reset()
     next_o = None
     path_length = 0
     if render:
         env.render(**render_kwargs)
     while path_length < max_path_length:
+        print ("sampler/rollout_functions.py, rollout, o: ", type(o), o.shape)
         a, agent_info = agent.get_action(o)
         next_o, r, d, env_info = env.step(a)
         observations.append(o)
diff --git a/rlkit/torch/networks.py b/rlkit/torch/networks.py
index f8be3d8..9b929d5 100644
--- a/rlkit/torch/networks.py
+++ b/rlkit/torch/networks.py
@@ -108,10 +108,13 @@ class MlpPolicy(Mlp, Policy):
         return super().forward(obs, **kwargs)
 
     def get_action(self, obs_np):
+        print ("torch/networks.py, get_action, obs_np: ", type(obs_np), ", ", obs_np.shape)
         actions = self.get_actions(obs_np[None])
+        print ("torch/networks.py, get_action, actions: ", type(actions), ", ", actions.shape)
         return actions[0, :], {}
 
     def get_actions(self, obs):
+        print ("torch/networks.py, get_actions, actions: ", type(obs), ", ", obs.shape)
         return eval_np(self, obs)
 
 
